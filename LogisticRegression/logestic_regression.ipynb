{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitmlnotebooksvenv73219d5a04ca4eef95a9c3672d8f471d",
   "display_name": "Python 3.8.5 64-bit ('MLNotebooks': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Logistic Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Introduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Logestic regression models the probability whether a data point belongs to a class $a$ or $b$, a binary classification model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The basic motivation for logestic regression is to answer the following question. Can the linear regression model be used to discriminate data into two classes a and b? Consider the following approach: After the model wieghts are learned, the model can be used to predict a value for a given data point. If this value is greater than a threshold, then it belongs to class a, otherwise, it belongs to the other class, b. A main cavieat with this approach is the determination of the threshold. Generally, linear regression models are not bounded within a specific range, say \\[0,1\\]. Hence, it is difficult to set such threshold. In this sense, logestic regression can be viewed as a classification extension to linear regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "What logestic regression offers is a way to artificialize such threshold. This is done by crashing values of any linear regression model into a range between \\[0,1\\] using a special function called sigmoid. The formula of this function is: $$\\sigma(z) = \\frac{1}{1+e^{-z}} \\longrightarrow (1)$$ This function also has a nice derivative form. \n",
    "$$ \\sigma(z) = (1+e^{-z})^{-1} $$\n",
    "$$ \\Longrightarrow \\frac{d \\sigma(z)}{dz} = -1(1+e^{-z})^{-2}(-e^{-z}) = \\frac{-e^{-z}}{-(1+e^{-z})^2}$$\n",
    "$$ \\Longrightarrow \\frac{1}{a+e^{-z}} . \\frac{e^{-z}}{1+e^{-z}}$$\n",
    "$$ \\Longrightarrow \\sigma(z) . \\frac{1+e^{-z}-1}{1+e^{-z}} $$\n",
    "$$ \\Longrightarrow \\sigma(z) . (1 - \\frac{1}{1+e^{-z}} $$\n",
    "$$ \\Longrightarrow \\sigma(z) . (1 - \\sigma(z)) \\longrightarrow (2) $$\n",
    "\n",
    "This form will be addressed later on this article."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The Forward Pass"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The general structure of a logestic unit is illustrated in the following graph. The input to the logestic unit are the data points $x_{1...m}$ and the outputs is a vector of biases added to the product of the weights and the features and transformed with sigmoid function for each data point.\n",
    "\n",
    "![title](d1.png)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Here are some points to highlight on the previous graph:\n",
    "\n",
    "- $X$ is matrix of data points features. Assumming that the number of features are $n$ and the number of data points are $m$, then the shape of the matrix should be a combination of both. \n",
    "\n",
    "- $W$ is a matrix of weights for each feature in each data point. If we have $m$ data points and $n$ features, then the shape of $W$ is a combination of both.\n",
    "\n",
    "- The operation between $W$ and $X$ is a dot product. The output of this operation should be a vector of length equal to the number of data points, $m$. Each element in this vector is the dot product of vectors $$(w_i)_k,(x_i)_k$$ where $w_i$ is the wieght of feature $i$ and the $x_i$ is the value of the feature $i$ for the data point $k$\n",
    "\n",
    "- In practice, the $X$ matrix, can take the shape $(n,m)$, and $W$ $(n,m)$. To apply the dot product, $W$ can be transposed and vectorization can be applied for this operation on the two matrixes in parallel. Python numpy library is an example of a tool that achieve this operation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A single forward pass charachterizes the model performance with the current weights. This charachterization happens througth the cost function. Just like in linear regression where the model performance cost is charachterized by the least mean square error formula. Logestic regression, thereafter, is charachterized by a different formula as follows: $$L = y\\log{\\hat{y}} - (1-y)\\log{(1-\\hat{y})}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "__Why logestic regression cost is different from linear regression?__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This is due to the fact logistic regression model depends on sigmoid function. This function is not linear. Applying the least square error metric $(\\hat{y}-y)^2$ would not result in a convext function. Non-convex function are hard to optimize due to many local minima they have."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "__Why this cost function specifically? Why not something else?__"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The root reason behind using this cost function recurse back to a statistical framework for finding the probability distribution of a given instances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}