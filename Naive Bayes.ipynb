{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Gentle Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem is an incredible formula. It can be used to infer on canditinal propabilities. it was named after the british mathematician, phillosopher and theologist _Thomas Bayes_. He has written two books one of which where this formula has been found. Bayes discovered it according to some phillosophical reasoning. He concluded that human vision of the world shall be updated incremenatlly according to repeated evidence occurances. The mathematical incredibility of this formula was figured out later where Bayes body was, unfortunately, undergrounded.\n",
    "\n",
    "Given two events A and B, The formula states:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A|B) = \\frac{P(B|A).P(A)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The term $P(A|B)$ is called the __Posterior__. It is the term of interest.\n",
    "- The term $P(B|A)$ is called the __Likelyhood__ or __Class Conditional Propability__. It measures how much likely (probable) the event B will happen given that the event A has happend.\n",
    "- The term $P(A)$ is called the __Prior__ probability. It describes the prior believe of the conditioned event \"in that case, A\". It is usually obtained from a filed expert or just by calculating its frequency from the given data.\n",
    "- Finally, the term P(B) is called the __Evidence__. It is a marginal probability independed of the class-conditional probability.\n",
    "- As the __Evidence__ term is independent of class conditional probabilites and is constant for all B's \" if B is a vector of values\", it can be neglected. sometimes it is called a scaling factor. In that case, the rule can be viewed as a direct proportion between the $P(A|B)$ and $P(B|A).P(A)$. Moreover, if the probability destribution of the event A is uniform \"all values have the same probability to occure\" then the value of the posteroir term is only determined by  $P(B|A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a basic knowledge of statistics and probability, the derivation of this formula is pritty simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Derivation of Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two propabilities, A and B, the definition of independence states the following:\n",
    "$P(A|B)$ = $\\frac{P(A,B)}{P(B)}$, where P(A,B) is the joint probability equevalent to the intersection between two events. But\n",
    "$P(B|A) = \\frac{P(A,B)}{P(A)}$ and $ P(A,B) = P(B|A).P(A)$, substituting in the first equation:\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Bayes Theorem, A Probabilistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although _Bayes Theorem_ is very simple, fundamental and useful, it is, nonetheless, counter intuitive! The following example will try to illustrate.\n",
    "\n",
    "### 3.1 Example\n",
    "A course in an A university is relatively simple. The course's letter grade is determined only by the final exam score. Although the course material is easy, it requires a fair amout of study. The instructor, based on his experience in this course, informs his students that 90% of students who got an A grade, had studied more than 5 hours for the final exam. On the other hand, there is no A-grade student who studied for less than that amout of time. However, students takes this exam unseriously. Statistics from previous semesters show that only 20% of students per class had passed the course with an A grade! Given that a student has studied 5 hours for this final exam, __what is the probability he will pass the course with an A grade?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in a first attempt, someone might answer that the probability shall be 90%. However, according to Bayes theorem, _this is not true!_ This is the probability of studing for 5 hours given that the student got an A grade. Not the converge of this condition. This is a common mistake of interpretating the conditional probability called _Base Rate Fallacy_\n",
    "\n",
    "Let us formulate the problem mathematically.\n",
    "\n",
    "Let P(A) be the probability the student will pass the course with an A grade. Statistics, or our prior believes, provides the value of this probability as 0.2.\n",
    "\n",
    "The likelyhood that the student had studied for 5 hours given that he got an A grade, P(H|A) is 0.90.\n",
    "\n",
    "P(H) can be interpreted as the probability of obtaining an A grade independent of the amount of the study time. It is called the __Marginal Probability__ and can be calculated as the probability of obtaining an A grade independet of the amount of time spent. Mathematically speacking: $P(H) = P(H|A)P(A) + P(H|\\bar A)P(\\bar A)$\n",
    "\n",
    "then, the probability the student will pass the course with an A grade given that he has studied for 5 hours would be:\n",
    "\n",
    "$$P(A|H) = \\frac{P(H|A).P(A)}{P(H)}$$\n",
    "\n",
    "plug in the numbers:\n",
    "\n",
    "$$P(A|H) = \\frac{0.9 * 0.2}{0.9 * 0.2 + 0.1 * 0.8} = 69\\%$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Bayes Theorem does not always imply counter intuitivity. Actually, the term controlling that is the prior term. Human minds tend to neglect the effect of this term. Nevertheless, it has a large effect on the final result of the posterior probability in interest. Returning back to the previous example, if the ratio of students who got an A grade were doubled to 40%, then the value of $P(A|H)$ would jump to $80\\%$! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4 Bayes Theorem in Classification, Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes was initially introduced to find the _posterior distribution_. Unlike other statistical methods that can found eithr a point estimate such as regression or confidence interval. The concept is then generalized to learn such distributions from a given data and use these distributions to predict on new data instances. In other words, to do classificaiton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any classification problem, a desicion boundary function is to be learned from the given data. For a Bayes Theorem to be employed in a classification algorithm, this function, sometimes called the hypothesis, need to be well formulated. The formulation might be stated as follows:\n",
    "\n",
    "$$C = argmax_{i\\in\\{1..n\\}}P(C_i|X)=argmax_{i\\in\\{1..n\\}}\\frac{P(X|C_i)* P(C_i)}{P(X)}$$\n",
    "\n",
    "__Where:__\n",
    "- $n$ is the total number of class labels.\n",
    "- $C$ is the predicted class label.\n",
    "- $P(C|X)$: the probability the given instance X belongs to the class label $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Classification algorithm emplying any Bayes theorem is called _Naive Bayes Algorithm_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When to use Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any Naive Bayes based Algorithm assumes that:\n",
    "- data is linearly separable. The desicion boundary learned by this algorithm is linear.\n",
    "- all data instances are _independent and identically distributed_ or _iid_. Many other classification algorithms rely on this assumption. If this assumption is violated, then the generalization error is unpredictable. Also, the prediction of the algorithm is hugely dependent on the training set.\n",
    "- all features are independent. This is why it is called _Naive_. Although this assumption seems very unreallastic, the algorithm found to perform very well. It may, but not necessarily, outperform some well known, competitive algorithms in this area such as SVMs and Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 Naive Bayes Algorithm, Brief Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you are given a dataset consisting of _N_ features, _K_ classes and _M_ instances. The task is then to find the posterior probability for each class using bayes theorem and nominate the maxixmum value among all probabilites as the predicted class.\n",
    "\n",
    "In order to use bayes theorem, the following terms shall be found: The prior probability and the class conditional probability.\n",
    "\n",
    "- The prior probability for each class can be calculated as follows:\n",
    "$$P(C_i) = \\frac{M_i}{M}, i\\in \\{0,1,,,K\\}$$\n",
    "__Where:__\n",
    "    - $C_i$, The class i.\n",
    "    - $M_i$, The number of data instances labeled as $C_i$.\n",
    "    - $M$, The total number of all data instances.\n",
    "    \n",
    "- The class conditional probability can be calculated \n",
    "- The evidence term can be neglected as it is reduced to a constant value for all classes. However, it can be calculated as follows:\n",
    "$$P(X) = \\sum_{i=1}^k{P(X|C_i).P(C_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
